\chapter{Introduction}\label{sec:intro}

In recent years, deep learning models have become larger and increasingly demand more data to perform their tasks satisfactorily. 
However, few-shot learning has garnered increasing interest recently because it underscores a fundamental gap between smart human adaptability and data-hungry supervised and unsupervised deep learning methods.

Today, machine learning models are capable of performing exceptionally well on a variety of tasks. Machine learning models already perform better than humans in several image related tasks such as object recognition, image super-resolution scaling, image analysis tasks in medicine and self-driving cars. Not only this, we have seen an explosive growth of large language models such as GPT-3 \parencite{brown2020language} and LaMDA \parencite{cohen2022lamda}, that are capable of generating coherent sentences in a manner that was previously not seen.
\begin{figure}[ht]
    \centering
    \includegraphics[scale=0.25]{chapters/assets/human-understanding.png}
    \caption{As a human you can immediately figure out which object is most similar to the one bounded by a \textcolor{red}{red box}, but machines are quite poor at this type of abstraction and learning. Image borrowed from \parencite{Lake2015Human-levelInduction}.}
    \label{fig:human-understanding}
\end{figure}

However, even while being cognisant of these achievements, we must realise that most of the leading approaches today (and state-of-the-art approaches from a few years ago) are also one of the most data-hungry and parameter heavy approaches. To put this hunger into perspective, SEER \parencite{Goyal2021} uses a model with $1.3$ billion parameters and trains it with SWaV \parencite{caron2020unsupervised}. However, this number is not even close to the league in which large language models (LLMs) operate. GPT-3 \parencite{brown2020language} has $175$-billion parameters and this number has only increased. Furthermore, such models require massive datasets such as ImageNet \parencite{deng2009imagenet} for image models and datasets such as The Pile \parencite{gao2020pile} for language models.

On the contrary, humans possess two crucial aspects of \textbf{conceptual knowledge}: first, humans generally require only a few examples from which they can learn natural or man-made concepts and categories, whereas machine learning models, as we discussed, require excessive amounts of examples to perform at a similar level.
Second, humans learn far richer abstractions that machine learning models do not, and are capable of utilising these abstractions for a wide range of functions. A good example of this in action is when children acquire language; for instance, for many verbs in English the past tense is formed by adding \textquote{-ed} to the verb's stem (e.g. walked). Although children may not explicitly learn the past tense forms for every such word, they can still naturally regularise (and generalise) the past tense form and reuse it as required \parencite{marcus1992overregularization}. 
Similarly, a human child can easily figure out what a car should look
Furthermore, humans can use their abstractions to generate new examples, parsing objects into their parts, and above all creating new abstractions based on existing ideas and abstractions. Machine learning models, on the other hand, struggle with generalisation and learning from fewer examples.

