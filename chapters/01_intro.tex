\chapter{Introduction}\label{sec:intro}

In recent years we have seen deep learning models grow larger and demand increasingly more data to perform their tasks satisfactorily. 
On the other-hand few-shot learning has been gathering increasing interest recently because it underscores a fundamental gap between smart human adaptability and data-hungry supervised and unsupervised deep learning methods. To tackle this challenge, few-shot classification is cast as a task of predicting class labels for a set of unlabelled data points (\textit{query set}) given only a small set of labelled data points (\textit{support set}). Typically, the query and support data points are drawn from the same distribution. 

Few-shot classification methods typically comprise of two sequential phases: (i) \textit{pre-training} on a large dataset of ``base'' classes, regardless of the training being supervised or unsupervised. This is followed by (ii) \textit{fine-tuning} on an unseen dataset consisting of ``novel'' classes. Normally, the classes used in the pre-training and fine-tuning are mutually exclusive. In this paper, our focus is on the self-supervised (also sometimes interchangeably called ``unsupervised'' in the literature) setting where we have no access to the actual class labels of the ``base'' dataset.

To this end, various methods have been proposed and broadly categorized under two different approaches. The first approach relies on using \textit{meta-learning} and episodic training that involves creating synthetic ``tasks'' to mimic the downstream episodic fine-tuning phase \parencite{Finn2017Model-agnosticNetworks, Hsu2018UnsupervisedMeta-Learning, Khodadadeh2018UnsupervisedClassification, Antoniou2019AssumeAugmentation, Ye2022, lee2021meta, Ji2019UnsupervisedTraining}. 
The second method follows a \textit{transfer learning} approach, where the network trained non-episodically to learn optimal representations in the pre-training phase, which is then followed by an episodic fine-tuning phase \parencite{Medina2020Self-SupervisedClassification, goodemballneed2020, dhillon2019baseline}.
In this method, a feature extractor (encoder) is trained using a form of metric learning to capture the structure of the unlabelled data. 
Next, a simple predictor (conventionally a linear layer) is utilised in conjunction with the pre-trained feature extractor for quick adaptation to the novel classes in the fine-tuning phase.
The better the feature extractor captures the global structure of the unlabelled data, the less the predictor requires training samples and the faster it adapts itself to the unseen classes in the fine-tuning phase (also the testing phase).

Furthermore, supervised approaches that follow the episodic training paradigm may include a certain degree of \textit{task awareness}.
Such approaches exploit the information available in the query set during the training or testing phases \parencite{bateni2022enhancing, ye2020few, Cui2021} to alleviate the model's sample bias. As a results of this, task-awareness allows the model to learn task-specific embeddings by better aligning the features of the support and query samples.
We also see a set of supervised approaches that do not rely purely on a convolutional feature extractor. Instead, these approaches also make use of graphs and graph neural networks \parencite{garcia2018fewshot, kim2019edge, yu2022hybrid, yang2020dpgn}. Using a graph neural network (GNN) can aid in modelling instance-level and class-level relationships. GNN's can also help propagate labels by using a task-agnostic classifier and such methods have been shown to work quite effectively when compared against their standard convolutional counterparts \parencite{kim2019edge, garcia2018fewshot, yu2022hybrid, yang2020dpgn}. However, graph based methods have eluded the unsupervised setting.

Several recent studies have questioned the necessity of meta-learning for few-shot classification \parencite{goodemballneed2020, Medina2020Self-SupervisedClassification, dhillon2019baseline, ziko2020laplacian, boudiaf2020information,chen2021self, shirekar2022self}. They report competitive performance on few-shot benchmarks without episodic training or few-shot task-based experiences during training. These methods follow the second approach and aim to solve the few-shot learning problem by fine-tuning a pre-trained feature extractor with a standard cross-entropy loss.
Some of these methods \parencite{Medina2020Self-SupervisedClassification, goodemballneed2020, das2022confess} in the space demonstrate that the transfer learning approach outperforms meta-learning based methods in standard in-domain and cross-domain settings - where the training and novel classes come from totally different distributions.