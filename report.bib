@article{monge1781memoire,
	title        = {M{\'e}moire sur la th{\'e}orie des d{\'e}blais et des remblais},
	author       = {Monge, Gaspard},
	year         = 1781,
	journal      = {Mem. Math. Phys. Acad. Royale Sci.},
	pages        = {666--704}
}
@article{hitchcock1941distribution,
	title        = {The distribution of a product from several sources to numerous localities},
	author       = {Hitchcock, Frank L},
	year         = 1941,
	journal      = {Journal of mathematics and physics},
	publisher    = {Wiley Online Library},
	volume       = 20,
	number       = {1-4},
	pages        = {224--230}
}
@article{Kantorovich42,
	title        = {On the transfer of masses (in Russian)},
	author       = {{Leonid Kantorovich}},
	year         = 1942,
	journal      = {Doklady Akademii Nauk},
	address      = {Hingham, MA, USA},
	volume       = 37,
	number       = 2,
	pages        = {227--229},
	optannote    = {Translated in Management Science, Vol. 5, pp. 1--4, 1959}
}
@article{mcculloch1943logical,
	title        = {A logical calculus of the ideas immanent in nervous activity},
	author       = {McCulloch, Warren S and Pitts, Walter},
	year         = 1943,
	journal      = {The bulletin of mathematical biophysics},
	publisher    = {Springer},
	volume       = 5,
	number       = 4,
	pages        = {115--133}
}
@article{Robbins1951,
	title        = {A Stochastic Approximation Method},
	author       = {Herbert Robbins and Sutton Monro},
	year         = 1951,
	month        = 9,
	journal      = {The annals of mathematical statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 22,
	pages        = {400--407},
	doi          = {10.1214/AOMS/1177729586},
	issn         = {0003-4851},
	url          = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full},
	abstract     = {Let $M(x)$ denote the expected value at level $x$ of the response to a certain experiment. $M(x)$ is assumed to be a monotone function of $x$ but is unknown to the experimenter, and it is desired to find the solution $x = \theta$ of the equation $M(x) = \alpha$, where $\alpha$ is a given constant. We give a method for making successive experiments at levels $x_1,x_2,\cdots$ in such a way that $x_n$ will tend to $\theta$ in probability.},
	issue        = 3
}
@article{Kiefer1952,
	title        = {Stochastic Estimation of the Maximum of a Regression Function},
	author       = {J. Kiefer and J. Wolfowitz},
	year         = 1952,
	month        = 9,
	journal      = {The annals of mathematical statistics},
	publisher    = {Institute of Mathematical Statistics},
	volume       = 23,
	pages        = {462--466},
	doi          = {10.1214/AOMS/1177729392},
	issn         = {0003-4851},
	url          = {https://projecteuclid-org.tudelft.idm.oclc.org/journals/annals-of-mathematical-statistics/volume-23/issue-3/Stochastic-Estimation-of-the-Maximum-of-a-Regression-Function/10.1214/aoms/1177729392.full https://projecteuclid-org.tudelft.idm.oclc.org/journals/annals-of-mathematical-statistics/volume-23/issue-3/Stochastic-Estimation-of-the-Maximum-of-a-Regression-Function/10.1214/aoms/1177729392.short},
	abstract     = {Let $M(x)$ be a regression function which has a maximum at the unknown point $\theta. M(x)$ is itself unknown to the statistician who, however, can take observations at any level $x$. This paper gives a scheme whereby, starting from an arbitrary point $x_1$, one obtains successively $x_2, x_3, \cdots$ such that $x_n$ converges to $\theta$ in probability as $n \rightarrow \infty$.},
	issue        = 3
}
@article{Sinkhorn1967,
	title        = {{Concerning nonnegative matrices and doubly stochastic matrices}},
	author       = {Sinkhorn, Richard and Knopp, Paul},
	year         = 1967,
	journal      = {Pacific Journal of Mathematics},
	volume       = 21,
	number       = 2,
	pages        = {343--348},
	doi          = {10.2140/PJM.1967.21.343},
	issn         = {00308730},
	abstract     = {This paper is concerned with the condition for the convergence to a doubly stochastic limit of a sequence of matrices obtained from a nonnegative matrix A by alternately scaling the rows and columns of A and with the condition for the existence of diagonal matrices D1 and D2 with positive main diagonals such that D1AD2 is doubly stochastic. The result is the following. The sequence of matrices converges to a doubly stochastic limit if and only if the matrix A contains at least one positive diagonal. A necessary and sufficient condition that there exist diagonal matrices A and D2 with positive main diagonals such that D1AD2 is both doubly stochastic and the limit of the iteration is that A蠀O and each positive entry of A is contained in a positive diagonal. The form D1AD2 is unique, and A and D2 are unique up to a positive scalar multiple if and only if A is fully indecomposable. {\textcopyright} 1967 by Pacific Journal of Mathematics.},
	mendeley-groups = {Unsupervised Meta Learning/Thesis Cites}
}
@article{Fukushima1975,
	title        = {Cognitron: A self-organizing multilayered neural network},
	author       = {Kunihiko Fukushima},
	year         = 1975,
	month        = 9,
	journal      = {Biological Cybernetics 1975 20:3},
	publisher    = {Springer},
	volume       = 20,
	pages        = {121--136},
	doi          = {10.1007/BF00342633},
	issn         = {1432-0770},
	url          = {https://link-springer-com.tudelft.idm.oclc.org/article/10.1007/BF00342633},
	abstract     = {A new hypothesis for the organization of synapses between neurons is proposed: “The synapse from neuron x to neuron y is reinforced when x fires provided that no neuron in the vicinity of y is firing stronger than y”. By introducing this hypothesis, a new algorithm with which a multilayered neural network is effectively organized can be deduced. A self-organizing multilayered neural network, which is named “cognitron”, is constructed following this algorithm, and is simulated on a digital computer. Unlike the organization of a usual brain models such as a three-layered perceptron, the self-organization of a cognitron progresses favorably without having a “teacher” which instructs in all particulars how the individual cells respond. After repetitive presentations of several stimulus patterns, the cognitron is self-organized in such a way that the receptive fields of the cells become relatively larger in a deeper layer. Each cell in the final layer integrates the information from whole parts of the first layer and selectively responds to a specific stimulus pattern or a feature.},
	issue        = 3,
	keywords     = {Bioinformatics,Complex Systems,Computer Appl. in Life Sciences,Neurobiology,Neurosciences},
	pmid         = 1203338
}
@incollection{fukushima1982neocognitron,
	title        = {Neocognitron: A self-organizing neural network model for a mechanism of visual pattern recognition},
	author       = {Fukushima, Kunihiko and Miyake, Sei},
	year         = 1982,
	booktitle    = {Competition and cooperation in neural nets},
	publisher    = {Springer},
	pages        = {267--285}
}
@article{Micchelli1986,
	title        = {Interpolation of scattered data: Distance matrices and conditionally positive definite functions},
	author       = {Charles A. Micchelli},
	year         = 1986,
	month        = 12,
	journal      = {Constructive Approximation 1986 2:1},
	publisher    = {Springer},
	volume       = 2,
	pages        = {11--22},
	doi          = {10.1007/BF01893414},
	issn         = {1432-0940},
	url          = {https://link-springer-com.tudelft.idm.oclc.org/article/10.1007/BF01893414},
	abstract     = {Among other things, we prove that multiquadric surface interpolation is always solvable, thereby settling a conjecture of R. Franke.},
	issue        = 1,
	keywords     = {Analysis,Multivariate interpolation,Numerical Analysis,Positive definite functions,Thin plate splines}
}
@article{rumelhart1986learning,
	title        = {Learning representations by back-propagating errors},
	author       = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year         = 1986,
	journal      = {nature},
	publisher    = {Nature Publishing Group},
	volume       = 323,
	number       = 6088,
	pages        = {533--536}
}
@mastersthesis{schmidhuber:1987:srl,
	title        = {Evolutionary Principles in Self-Referential Learning. On Learning now to Learn: The Meta-Meta-Meta...-Hook},
	author       = {Schmidhuber, Jurgen},
	year         = 1987,
	month        = {14 May},
	url          = {http://www.idsia.ch/~juergen/diploma.html},
	added-at     = {2008-06-19T17:46:40.000+0200},
	biburl       = {https://www.bibsonomy.org/bibtex/2a96f7c3d42103ab94b13badef5d869f0/brazovayeye},
	interhash    = {b1d12416bd2edc34c30961f0ae978d8f},
	intrahash    = {a96f7c3d42103ab94b13badef5d869f0},
	keywords     = {EURISKO, PSALM, SALM, algorithm, algorithms, associative brigade, bucket evolution, fractals genetic genetical introsepection, learning, meta, nets, neuronal programming self-reference,},
	school       = {Technische Universitat Munchen, Germany},
	size         = {62 pages},
	timestamp    = {2008-06-19T17:51:06.000+0200},
	type         = {Diploma Thesis}
}
@article{Pinker1988,
	title        = {On language and connectionism: Analysis of a parallel distributed processing model of language acquisition},
	author       = {Steven Pinker and Alan Prince},
	year         = 1988,
	month        = 3,
	journal      = {Cognition},
	publisher    = {Elsevier},
	volume       = 28,
	pages        = {73--193},
	doi          = {10.1016/0010-0277(88)90032-7},
	issn         = {0010-0277},
	abstract     = {Does knowledge of language consist of mentally-represented rules? Rumelhart and McClelland have described a connectionist (parallel distributed processing) model of the acquisition of the past tense in English which successfully maps many stems onto their past tense forms, both regular (walk/walked) and irregular (go/went), and which mimics some of the errors and sequences of development of children. Yet the model contains no explicit rules, only a set of neuronstyle units which stand for trigrams of phonetic features of the stem, a set of units which stand for trigrams of phonetic features of the past form, and an array of connections between the two sets of units whose strengths are modified during learning. Rumelhart and McClelland conclude that linguistic rules may be merely convenient approximate fictions and that the real causal processes in language use and acquisition must be characterized as the transfer of activation levels among units and the modification of the weights of their connections. We analyze both the linguistic and the developmental assumptions of the model in detail and discover that (1) it cannot represent certain words, (2) it cannot learn many rules, (3) it can learn rules found in no human language, (4) it cannot explain morphological and phonological regularities, (5) it cannot explain the differences between irregular and regular forms, (6) it fails at its assigned task of mastering the past tense of English, (7) it gives an incorrect explanation for two developmental phenomena: stages of overregularization of irregular forms such as bringed, and the appearance of doubly-marked forms such as ated and (8) it gives accounts of two others (infrequent overregularization of verbs ending in t/d, and the order of acquisition of different irregular subclasses) that are indistinguishable from those of rule-based theories. In addition, we show how many failures of the model can be attributed to its connectionist architecture. We conclude that connectionists' claims about the dispensability of rules in explanations in the psychology of language must be rejected, and that, on the contrary, the linguistic and developmental facts provide good evidence for such rules. © 1988.},
	issue        = {1-2},
	pmid         = 2450717
}
@article{Cybenko1989,
	title        = {Approximation by superpositions of a sigmoidal function},
	author       = {G. Cybenko},
	year         = 1989,
	month        = 12,
	journal      = {Mathematics of Control, Signals and Systems 1989 2:4},
	publisher    = {Springer},
	volume       = 2,
	pages        = {303--314},
	doi          = {10.1007/BF02551274},
	issn         = {1435-568X},
	url          = {https://link-springer-com.tudelft.idm.oclc.org/article/10.1007/BF02551274},
	abstract     = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
	issue        = 4,
	keywords     = {Communications Engineering,Control,Mechatronics,Networks,Robotics,Systems Theory}
}
@article{LeCun1989,
	title        = {Backpropagation Applied to Handwritten Zip Code Recognition},
	author       = {Y. LeCun and B. Boser and J. S. Denker and D. Henderson and R. E. Howard and W. Hubbard and L. D. Jackel},
	year         = 1989,
	month        = 12,
	journal      = {Neural Computation},
	publisher    = {MIT Press - Journals},
	volume       = 1,
	pages        = {541--551},
	doi          = {10.1162/NECO.1989.1.4.541},
	issn         = {0899-7667},
	abstract     = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
	issue        = 4
}
@article{Hornik1991,
	title        = {Approximation capabilities of multilayer feedforward networks},
	author       = {Kurt Hornik},
	year         = 1991,
	month        = 1,
	journal      = {Neural Networks},
	publisher    = {Pergamon},
	volume       = 4,
	pages        = {251--257},
	doi          = {10.1016/0893-6080(91)90009-T},
	issn         = {0893-6080},
	abstract     = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(μ) performance criteria, for arbitrary finite input environment measures μ, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. © 1991.},
	issue        = 2,
	keywords     = {Activation function,Input environment measure,Lp(μ) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities}
}
@article{Brenier1991PolarFunctions,
	title        = {{Polar factorization and monotone rearrangement of vector-valued functions}},
	author       = {Brenier, Yann},
	year         = 1991,
	month        = 6,
	journal      = {Communications on Pure and Applied Mathematics},
	publisher    = {John Wiley {\&} Sons, Ltd},
	volume       = 44,
	number       = 4,
	pages        = {375--417},
	doi          = {10.1002/CPA.3160440402},
	issn         = {1097-0312},
	url          = {https://onlinelibrary-wiley-com.tudelft.idm.oclc.org/doi/full/10.1002/cpa.3160440402 https://onlinelibrary-wiley-com.tudelft.idm.oclc.org/doi/abs/10.1002/cpa.3160440402 https://onlinelibrary-wiley-com.tudelft.idm.oclc.org/doi/10.1002/cpa.3160440402}
}
@article{marcus1992overregularization,
	title        = {Overregularization in language acquisition},
	author       = {Marcus, Gary F and Pinker, Steven and Ullman, Michael and Hollander, Michelle and Rosen, T John and Xu, Fei and Clahsen, Harald},
	year         = 1992,
	journal      = {Monographs of the society for research in child development},
	publisher    = {JSTOR},
	pages        = {i--178}
}
@inproceedings{deng2009imagenet,
	title        = {Imagenet: A large-scale hierarchical image database},
	author       = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year         = 2009,
	booktitle    = {2009 IEEE conference on computer vision and pattern recognition},
	pages        = {248--255},
	organization = {Ieee}
}
@article{Kriegel09,
	title        = {Clustering High-Dimensional Data: A Survey on Subspace Clustering, Pattern-Based Clustering, and Correlation Clustering},
	author       = {Kriegel, Hans-Peter and Kr\"{o}ger, Peer and Zimek, Arthur},
	year         = 2009,
	month        = {mar},
	journal      = {ACM Trans. Knowl. Discov. Data},
	publisher    = {Association for Computing Machinery},
	address      = {New York, NY, USA},
	volume       = 3,
	number       = 1,
	doi          = {10.1145/1497577.1497578},
	issn         = {1556-4681},
	url          = {https://doi-org.tudelft.idm.oclc.org/10.1145/1497577.1497578},
	issue_date   = {March 2009},
	abstract     = {As a prolific research area in data mining, subspace clustering and related problems induced a vast quantity of proposed solutions. However, many publications compare a new proposition—if at all—with one or two competitors, or even with a so-called “na\"{\i}ve” ad hoc solution, but fail to clarify the exact problem definition. As a consequence, even if two solutions are thoroughly compared experimentally, it will often remain unclear whether both solutions tackle the same problem or, if they do, whether they agree in certain tacit assumptions and how such assumptions may influence the outcome of an algorithm. In this survey, we try to clarify: (i) the different problem definitions related to subspace clustering in general; (ii) the specific difficulties encountered in this field of research; (iii) the varying assumptions, heuristics, and intuitions forming the basis of different approaches; and (iv) how several prominent solutions tackle different problems.},
	articleno    = 1,
	numpages     = 58,
	keywords     = {high-dimensional data, clustering, Survey}
}
@techreport{WelinderEtal2010,
	title        = {{Caltech-UCSD Birds 200}},
	author       = {P. Welinder and S. Branson and T. Mita and C. Wah and F. Schroff and S. Belongie and P. Perona},
	year         = 2010,
	number       = {CNS-TR-2010-001},
	institution  = {California Institute of Technology}
}
@techreport{WahCUB_200_2011,
	title        = {{The Caltech-UCSD Birds-200-2011 Dataset}},
	author       = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
	year         = 2011,
	number       = {CNS-TR-2011-001},
	institution  = {California Institute of Technology}
}
@article{scikit-learn,
	title        = {Scikit-learn: Machine Learning in {P}ython},
	author       = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year         = 2011,
	journal      = {Journal of Machine Learning Research},
	volume       = 12,
	pages        = {2825--2830}
}
@inproceedings{AlexNet2012,
	title        = {ImageNet Classification with Deep Convolutional Neural Networks},
	author       = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year         = 2012,
	booktitle    = {Advances in Neural Information Processing Systems},
	publisher    = {Curran Associates, Inc.},
	volume       = 25,
	pages        = {},
	url          = {https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
	editor       = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger}
}
@article{cuturi2013sinkhorn,
	title        = {Sinkhorn distances: Lightspeed computation of optimal transport},
	author       = {Cuturi, Marco},
	year         = 2013,
	journal      = {Advances in neural information processing systems},
	volume       = 26
}
@article{Zeiler2013,
	title        = {Visualizing and Understanding Convolutional Networks},
	author       = {Matthew D. Zeiler and Rob Fergus},
	year         = 2013,
	month        = 11,
	journal      = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher    = {Springer Verlag},
	volume       = {8689 LNCS},
	pages        = {818--833},
	doi          = {10.48550/arxiv.1311.2901},
	isbn         = 9783319105895,
	issn         = 16113349,
	url          = {https://arxiv.org/abs/1311.2901v3},
	abstract     = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
	issue        = {PART 1}
}
@article{kingma2014adam,
	title        = {Adam: A method for stochastic optimization},
	author       = {Kingma, Diederik P and Ba, Jimmy},
	year         = 2014,
	journal      = {arXiv preprint arXiv:1412.6980}
}
@article{Lake2015Human-levelInduction,
	title        = {{Human-level concept learning through probabilistic program induction}},
	author       = {Lake, Brenden M. and Salakhutdinov, Ruslan and Tenenbaum, Joshua B.},
	year         = 2015,
	month        = 12,
	journal      = {Science},
	publisher    = {American Association for the Advancement of Science},
	volume       = 350,
	number       = 6266,
	pages        = {1332--1338},
	doi          = {10.1126/SCIENCE.AAB3050/SUPPL{\_}FILE/LAKE-SM.PDF},
	issn         = 10959203,
	url          = {https://www.science.org},
	pmid         = 26659050
}
@article{He2015,
	title        = {Deep Residual Learning for Image Recognition},
	author       = {Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
	year         = 2015,
	month        = 12,
	url          = {http://arxiv.org/abs/1512.03385}
}
@article{dong2015image,
	title        = {Image super-resolution using deep convolutional networks},
	author       = {Dong, Chao and Loy, Chen Change and He, Kaiming and Tang, Xiaoou},
	year         = 2015,
	journal      = {IEEE transactions on pattern analysis and machine intelligence},
	publisher    = {IEEE},
	volume       = 38,
	number       = 2,
	pages        = {295--307}
}
@article{mohanty2016using,
	title        = {{Using Deep Learning for Image-Based Plant Disease Detection}},
	author       = {Mohanty, Sharada P and Hughes, David P and Salath{\'e}, Marcel},
	year         = 2016,
	journal      = {Frontiers in Plant Science},
	publisher    = {Frontiers},
	volume       = 7,
	pages        = 1419
}
@article{Vinyals2016MatchingLearning,
	title        = {Matching networks for one shot learning},
	author       = {Vinyals, Oriol and Blundell, Charles and Lillicrap, Timothy and Wierstra, Daan and others},
	year         = 2016,
	journal      = {Advances in neural information processing systems},
	volume       = 29
}
@article{Dumoulin2016,
	title        = {A guide to convolution arithmetic for deep learning},
	author       = {Vincent Dumoulin and Francesco Visin and George E P Box},
	year         = 2016,
	month        = 3,
	doi          = {10.48550/arxiv.1603.07285},
	url          = {https://arxiv.org/abs/1603.07285v2},
	abstract     = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.}
}
@article{Bottou2016,
	title        = {Optimization Methods for Large-Scale Machine Learning},
	author       = {Léon Bottou and Frank E. Curtis and Jorge Nocedal},
	year         = 2016,
	month        = 6,
	doi          = {10.48550/arxiv.1606.04838},
	url          = {https://arxiv.org/abs/1606.04838},
	abstract     = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.}
}
@book{GoodfellowDLBook2016,
	title        = {Deep Learning},
	author       = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	year         = 2016,
	publisher    = {MIT Press},
	note         = {\url{http://www.deeplearningbook.org}}
}
@article{kipf2016semi,
	title        = {Semi-supervised classification with graph convolutional networks},
	author       = {Kipf, Thomas N and Welling, Max},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1609.02907}
}
@article{donahue2016adversarial,
	title        = {Adversarial feature learning},
	author       = {Donahue, Jeff and Kr{\"a}henb{\"u}hl, Philipp and Darrell, Trevor},
	year         = 2016,
	journal      = {arXiv preprint arXiv:1605.09782}
}
@article{chen2016infogan,
	title        = {Infogan: Interpretable representation learning by information maximizing generative adversarial nets},
	author       = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	year         = 2016,
	journal      = {Advances in neural information processing systems},
	volume       = 29
}
@inproceedings{Finn2017Model-agnosticNetworks,
	title        = {Model-agnostic meta-learning for fast adaptation of deep networks},
	author       = {Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
	year         = 2017,
	booktitle    = {International conference on machine learning},
	pages        = {1126--1135},
	organization = {PMLR}
}
@article{garcia2017few,
	title        = {Few-shot learning with graph neural networks},
	author       = {Garcia, Victor and Bruna, Joan},
	year         = 2017,
	journal      = {arXiv preprint arXiv:1711.04043}
}
@misc{helber2017eurosat,
	title        = {EuroSAT: A Novel Dataset and Deep Learning Benchmark for Land Use and Land Cover Classification},
	author       = {Patrick Helber and Benjamin Bischke and Andreas Dengel and Damian Borth},
	year         = 2017,
	eprint       = {1709.00029},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{McInnes2017Hdbscan:Clustering,
	title        = {hdbscan: Hierarchical density based clustering.},
	author       = {McInnes, Leland and Healy, John and Astels, Steve},
	year         = 2017,
	journal      = {J. Open Source Softw.},
	volume       = 2,
	number       = 11,
	pages        = 205
}
@article{Snell2017PrototypicalLearning,
	title        = {Prototypical networks for few-shot learning},
	author       = {Snell, Jake and Swersky, Kevin and Zemel, Richard},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
@article{vaswani2017attention,
	title        = {Attention is all you need},
	author       = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
@inproceedings{wang2017chestx,
	title        = {Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases},
	author       = {Wang, Xiaosong and Peng, Yifan and Lu, Le and Lu, Zhiyong and Bagheri, Mohammadhadi and Summers, Ronald M},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {2097--2106}
}
@inproceedings{ZhongRe-rankingEncoding,
	title        = {Re-ranking person re-identification with k-reciprocal encoding},
	author       = {Zhong, Zhun and Zheng, Liang and Cao, Donglin and Li, Shaozi},
	year         = 2017,
	booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages        = {1318--1327}
}
@article{Eickenberg2017,
	title        = {Seeing it all: Convolutional network layers map the function of the human visual system},
	author       = {Michael Eickenberg and Alexandre Gramfort and Gaël Varoquaux and Bertrand Thirion},
	year         = 2017,
	month        = 5,
	journal      = {NeuroImage},
	publisher    = {Academic Press},
	volume       = 152,
	pages        = {184--194},
	doi          = {10.1016/J.NEUROIMAGE.2016.10.001},
	issn         = {1053-8119},
	abstract     = {Convolutional networks used for computer vision represent candidate models for the computations performed in mammalian visual systems. We use them as a detailed model of human brain activity during the viewing of natural images by constructing predictive models based on their different layers and BOLD fMRI activations. Analyzing the predictive performance across layers yields characteristic fingerprints for each visual brain region: early visual areas are better described by lower level convolutional net layers and later visual areas by higher level net layers, exhibiting a progression across ventral and dorsal streams. Our predictive model generalizes beyond brain responses to natural images. We illustrate this on two experiments, namely retinotopy and face-place oppositions, by synthesizing brain activity and performing classical brain mapping upon it. The synthesis recovers the activations observed in the corresponding fMRI studies, showing that this deep encoding model captures representations of brain function that are universal across experimental paradigms.},
	pmid         = 27777172
}
@article{zaheer2017deep,
	title        = {Deep sets},
	author       = {Zaheer, Manzil and Kottur, Satwik and Ravanbakhsh, Siamak and Poczos, Barnabas and Salakhutdinov, Russ R and Smola, Alexander J},
	year         = 2017,
	journal      = {Advances in neural information processing systems},
	volume       = 30
}
@inproceedings{garcia2018fewshot,
	title        = {Few-Shot Learning with Graph Neural Networks},
	author       = {Victor Garcia Satorras and Joan Bruna Estrach},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=BJj6qGbRW}
}
@article{ghiasi2018dropblock,
	title        = {Dropblock: A regularization method for convolutional networks},
	author       = {Ghiasi, Golnaz and Lin, Tsung-Yi and Le, Quoc V},
	year         = 2018,
	journal      = {Advances in neural information processing systems},
	volume       = 31
}
@article{Hsu2018UnsupervisedMeta-Learning,
	title        = {Unsupervised learning via meta-learning},
	author       = {Hsu, Kyle and Levine, Sergey and Finn, Chelsea},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1810.02334}
}
@article{liu2018learning,
	title        = {Learning to propagate labels: Transductive propagation network for few-shot learning},
	author       = {Liu, Yanbin and Lee, Juho and Park, Minseop and Kim, Saehoon and Yang, Eunho and Hwang, Sung Ju and Yang, Yi},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1805.10002}
}
@inproceedings{mishra2018a,
	title        = {A Simple Neural Attentive Meta-Learner},
	author       = {Nikhil Mishra and Mostafa Rohaninejad and Xi Chen and Pieter Abbeel},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=B1DmUzWAW}
}
@article{oreshkin2018tadam,
	title        = {Tadam: Task dependent adaptive metric for improved few-shot learning},
	author       = {Oreshkin, Boris and Rodr{\'\i}guez L{\'o}pez, Pau and Lacoste, Alexandre},
	year         = 2018,
	journal      = {Advances in neural information processing systems},
	volume       = 31
}
@article{ren2018meta,
	title        = {Meta-learning for semi-supervised few-shot classification},
	author       = {Ren, Mengye and Triantafillou, Eleni and Ravi, Sachin and Snell, Jake and Swersky, Kevin and Tenenbaum, Joshua B and Larochelle, Hugo and Zemel, Richard S},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1803.00676}
}
@inproceedings{velic018graph,
	title        = {Graph Attention Networks},
	author       = {Petar Veličković and Guillem Cucurull and Arantxa Casanova and Adriana Romero and Pietro Liò and Yoshua Bengio},
	year         = 2018,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=rJXMpikCZ}
}
@article{Kuzovkin2018,
	title        = {Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex},
	author       = {Ilya Kuzovkin and Raul Vicente and Mathilde Petton and Jean Philippe Lachaux and Monica Baciu and Philippe Kahane and Sylvain Rheims and Juan R. Vidal and Jaan Aru},
	year         = 2018,
	month        = 8,
	journal      = {Communications Biology 2018 1:1},
	publisher    = {Nature Publishing Group},
	volume       = 1,
	pages        = {1--12},
	doi          = {10.1038/S42003-018-0110-Y},
	issn         = {2399-3642},
	url          = {https://www-nature-com.tudelft.idm.oclc.org/articles/s42003-018-0110-y},
	abstract     = {Recent advances in the field of artificial intelligence have revealed principles about neural processing, in particular about vision. Previous work demonstrated a direct correspondence between the hierarchy of the human visual areas and layers of deep convolutional neural networks (DCNN) trained on visual object recognition. We use DCNN to investigate which frequency bands correlate with feature transformations of increasing complexity along the ventral visual pathway. By capitalizing on intracranial depth recordings from 100 patients we assess the alignment between the DCNN and signals at different frequency bands. We find that gamma activity (30–70 Hz) matches the increasing complexity of visual feature representations in DCNN. These findings show that the activity of the DCNN captures the essential characteristics of biological object recognition not only in space and time, but also in the frequency domain. These results demonstrate the potential that artificial intelligence algorithms have in advancing our understanding of the brain. Ilya Kuzovkin et al. compare intracranial depth recordings from human subjects taken during a visual recognition task to activations of deep convolutional neural networks (DCNNs). They find that signals in gamma frequency bands in the recordings are aligned with the hierarchical layer structure of the DCNN, showing that DCNNs capture important characteristics of biological object recognition.},
	issue        = 1,
	keywords     = {Learning algorithms,Object vision},
	pmid         = 30271987
}
@article{Murphy2018,
	title        = {Janossy Pooling: Learning Deep Permutation-Invariant Functions for Variable-Size Inputs},
	author       = {Ryan L. Murphy and Balasubramaniam Srinivasan and Bruno Ribeiro and Vinayak Rao},
	year         = 2018,
	month        = 11,
	journal      = {7th International Conference on Learning Representations, ICLR 2019},
	publisher    = {International Conference on Learning Representations, ICLR},
	url          = {https://arxiv.org/abs/1811.01900v3},
	abstract     = {We consider a simple and overarching representation for permutation-invariant functions of sequences (or multiset functions). Our approach, which we call Janossy pooling, expresses a permutation-invariant function as the average of a permutation-sensitive function applied to all reorderings of the input sequence. This allows us to leverage the rich and mature literature on permutation-sensitive functions to construct novel and flexible permutation-invariant functions. If carried out naively, Janossy pooling can be computationally prohibitive. To allow computational tractability, we consider three kinds of approximations: canonical orderings of sequences, functions with $k$-order interactions, and stochastic optimization algorithms with random permutations. Our framework unifies a variety of existing work in the literature, and suggests possible modeling and algorithmic extensions. We explore a few in our experiments, which demonstrate improved performance over current state-of-the-art methods.}
}
@article{gidaris2018unsupervised,
	title        = {Unsupervised representation learning by predicting image rotations},
	author       = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1803.07728}
}
@inproceedings{caron2018deep,
	title        = {Deep clustering for unsupervised learning of visual features},
	author       = {Caron, Mathilde and Bojanowski, Piotr and Joulin, Armand and Douze, Matthijs},
	year         = 2018,
	booktitle    = {Proceedings of the European conference on computer vision (ECCV)},
	pages        = {132--149}
}
@article{berthelot2018understanding,
	title        = {Understanding and improving interpolation in autoencoders via an adversarial regularizer},
	author       = {Berthelot, David and Raffel, Colin and Roy, Aurko and Goodfellow, Ian},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1807.07543}
}
@article{antoniou2018train,
	title        = {How to train your MAML},
	author       = {Antoniou, Antreas and Edwards, Harrison and Storkey, Amos},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1810.09502}
}
@article{mcinnes2018umap,
	title        = {Umap: Uniform manifold approximation and projection for dimension reduction},
	author       = {McInnes, Leland and Healy, John and Melville, James},
	year         = 2018,
	journal      = {arXiv preprint arXiv:1802.03426}
}
@article{Antoniou2019AssumeAugmentation,
	title        = {Assume, augment and learn: Unsupervised few-shot meta-learning via random labels and data augmentation},
	author       = {Antoniou, Antreas and Storkey, Amos},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1902.09884}
}
@article{bachman2019learning,
	title        = {Learning representations by maximizing mutual information across views},
	author       = {Bachman, Philip and Hjelm, R Devon and Buchwalter, William},
	year         = 2019,
	journal      = {Advances in neural information processing systems},
	volume       = 32
}
@article{dhillon2019baseline,
	title        = {A baseline for few-shot image classification},
	author       = {Dhillon, Guneet S and Chaudhari, Pratik and Ravichandran, Avinash and Soatto, Stefano},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1909.02729}
}
@article{guo2019new,
	title        = {{A New Benchmark for Evaluation of Cross-Domain Few-Shot Learning}},
	author       = {Guo, Yunhui and Codella, Noel CF and Karlinsky, Leonid and Smith, John R and Rosing, Tajana and Feris, Rogerio},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1912.07200}
}
@article{isic2018dataset,
	title        = {Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the international skin imaging collaboration (isic)},
	author       = {Codella, Noel and Rotemberg, Veronica and Tschandl, Philipp and Celebi, M Emre and Dusza, Stephen and Gutman, David and Helba, Brian and Kalloo, Aadi and Liopyris, Konstantinos and Marchetti, Michael and others},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1902.03368}
}
@article{Ji2019UnsupervisedTraining,
	title        = {Unsupervised few-shot learning via self-supervised training},
	author       = {Ji, Zilong and Zou, Xiaolong and Huang, Tiejun and Wu, Si},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1912.12178}
}
@article{Khodadadeh2018UnsupervisedClassification,
	title        = {Unsupervised meta-learning for few-shot image classification},
	author       = {Khodadadeh, Siavash and Boloni, Ladislau and Shah, Mubarak},
	year         = 2019,
	journal      = {Advances in neural information processing systems},
	volume       = 32
}
@inproceedings{kim2019edge,
	title        = {Edge-labeling graph neural network for few-shot learning},
	author       = {Kim, Jongmin and Kim, Taesup and Kim, Sungwoong and Yoo, Chang D},
	year         = 2019,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {11--20}
}
@inproceedings{Lee_2019_CVPR,
	title        = {Meta-Learning With Differentiable Convex Optimization},
	author       = {Lee, Kwonjoon and Maji, Subhransu and Ravichandran, Avinash and Soatto, Stefano},
	year         = 2019,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@article{peyre2019computational,
	title        = {Computational optimal transport: With applications to data science},
	author       = {Peyr{\'e}, Gabriel and Cuturi, Marco and others},
	year         = 2019,
	journal      = {Foundations and Trends{\textregistered} in Machine Learning},
	publisher    = {Now Publishers, Inc.},
	volume       = 11,
	number       = {5-6},
	pages        = {355--607}
}
@article{wang2019simpleshot,
	title        = {Simpleshot: Revisiting nearest-neighbor classification for few-shot learning},
	author       = {Wang, Yan and Chao, Wei-Lun and Weinberger, Kilian Q and van der Maaten, Laurens},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1911.04623}
}
@article{Lee2018,
	title        = {{Set Transformer: A Framework for Attention-Based Permutation-Invariant Neural Networks}},
	author       = {Lee, Juho and Lee, Yoonho and Kim, Jungtaek and Kosiorek, Adam and Choi, Seungjin and Teh, Yee Whye},
	year         = 2019,
	journal      = {International Conference on Machine Learning (ICML)}
}
@article{COTFNT,
	title        = {Computational Optimal Transport},
	author       = {Gabriel Peyr\’e and Marco Cuturi},
	year         = 2019,
	journal      = {Foundations and Trends in Machine Learning},
	volume       = 11,
	number       = {5-6},
	pages        = {355--607}
}
@article{asano2019self,
	title        = {Self-labelling via simultaneous clustering and representation learning},
	author       = {Asano, Yuki Markus and Rupprecht, Christian and Vedaldi, Andrea},
	year         = 2019,
	journal      = {arXiv preprint arXiv:1911.05371}
}
@incollection{pytorch2019,
	title        = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	author       = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year         = 2019,
	booktitle    = {Advances in Neural Information Processing Systems 32},
	publisher    = {Curran Associates, Inc.},
	pages        = {8024--8035},
	url          = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}
@article{wang2019dynamic,
	title        = {Dynamic graph cnn for learning on point clouds},
	author       = {Wang, Yue and Sun, Yongbin and Liu, Ziwei and Sarma, Sanjay E and Bronstein, Michael M and Solomon, Justin M},
	year         = 2019,
	journal      = {Acm Transactions On Graphics (tog)},
	publisher    = {ACM New York, NY, USA},
	volume       = 38,
	number       = 5,
	pages        = {1--12}
}
@inproceedings{zhang2019latentgnn,
	title        = {Latentgnn: Learning efficient non-local relations for visual recognition},
	author       = {Zhang, Songyang and He, Xuming and Yan, Shipeng},
	year         = 2019,
	booktitle    = {International Conference on Machine Learning},
	pages        = {7374--7383},
	organization = {PMLR}
}
@article{boudiaf2020information,
	title        = {Information maximization for few-shot learning},
	author       = {Boudiaf, Malik and Ziko, Imtiaz and Rony, J{\'e}r{\^o}me and Dolz, Jos{\'e} and Piantanida, Pablo and Ben Ayed, Ismail},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {2445--2457}
}
@article{caron2020unsupervised,
	title        = {Unsupervised learning of visual features by contrasting cluster assignments},
	author       = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
	year         = 2020,
	journal      = {Advances in Neural Information Processing Systems},
	volume       = 33,
	pages        = {9912--9924}
}
@inproceedings{chen2020simple,
	title        = {A simple framework for contrastive learning of visual representations},
	author       = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
	year         = 2020,
	booktitle    = {International conference on machine learning},
	pages        = {1597--1607},
	organization = {PMLR}
}
@inproceedings{goodemballneed2020,
	title        = {Rethinking few-shot image classification: a good embedding is all you need?},
	author       = {Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B and Isola, Phillip},
	year         = 2020,
	booktitle    = {European Conference on Computer Vision},
	pages        = {266--282},
	organization = {Springer}
}
@article{grill2020bootstrap,
	title        = {Bootstrap your own latent-a new approach to self-supervised learning},
	author       = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre and Buchatskaya, Elena and Doersch, Carl and Avila Pires, Bernardo and Guo, Zhaohan and Gheshlaghi Azar, Mohammad and others},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {21271--21284}
}
@inproceedings{he2020momentum,
	title        = {Momentum contrast for unsupervised visual representation learning},
	author       = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {9729--9738}
}
@article{Khodadadeh2020UnsupervisedModels,
	title        = {Unsupervised meta-learning through latent-space interpolation in generative models},
	author       = {Khodadadeh, Siavash and Zehtabian, Sharare and Vahidian, Saeed and Wang, Weijia and Lin, Bill and B{\"o}l{\"o}ni, Ladislau},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.10236}
}
@article{li2020few,
	title        = {Few-shot image classification via contrastive self-supervised learning},
	author       = {Li, Jianyi and Liu, Guizhong},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2008.09942}
}
@article{Medina2020Self-SupervisedClassification,
	title        = {Self-supervised prototypical transfer learning for few-shot classification},
	author       = {Medina, Carlos and Devos, Arnout and Grossglauser, Matthias},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2006.11325}
}
@article{Ouali2020SpatialClassification,
	title        = {{Spatial Contrastive Learning for Few-Shot Classification}},
	author       = {Ouali, Yassine and Hudelot, Céline and Tami, Myriam},
	year         = 2020,
	month        = 12,
	journal      = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	publisher    = {Springer Science and Business Media Deutschland GmbH},
	volume       = {12975 LNAI},
	pages        = {671--686},
	doi          = {10.1007/978-3-030-86486-6{\_}41},
	isbn         = 9783030864859,
	issn         = 16113349,
	url          = {https://arxiv.org/abs/2012.13831v3},
	arxivid      = {2012.13831},
	keywords     = {Contrastive learning, Deep learning, Few-shot learning}
}
@article{Qin2020DiversityAugmentation,
	title        = {Diversity helps: Unsupervised few-shot learning via distribution shift-based data augmentation},
	author       = {Qin, Tiexin and Li, Wenbin and Shi, Yinghuan and Gao, Yang},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2004.05805}
}
@inproceedings{Tian2020RethinkingNeed,
	title        = {{Rethinking Few-Shot Image Classification: A Good Embedding is All You Need?}},
	author       = {Tian, Yonglong and Wang, Yue and Krishnan, Dilip and Tenenbaum, Joshua B and Isola, Phillip},
	year         = 2020,
	booktitle    = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	volume       = {12359 LNCS},
	pages        = {266--282},
	doi          = {10.1007/978-3-030-58568-6{\_}16},
	isbn         = 9783030585679,
	issn         = 16113349,
	url          = {http://github.com/WangYueFt/},
	arxivid      = {2003.11539}
}
@article{Triantafillou2020Meta-Dataset:Examples,
	title        = {{Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples}},
	author       = {Triantafillou, Eleni and Zhu, Tyler and Dumoulin, Vincent and Lamblin, Pascal and Evci, Utku and Xu, Kelvin and Goroshin, Ross and Gelada, Carles and Swersky, Kevin and Manzagol, Pierre-Antoine and Larochelle, Hugo},
	year         = 2020,
	journal      = {arXiv preprint arXiv:1903.03096},
	url          = {http://arxiv.org/abs/1903.03096},
	arxivid      = {1903.03096}
}
@inproceedings{yang2020dpgn,
	title        = {Dpgn: Distribution propagation graph network for few-shot learning},
	author       = {Yang, Ling and Li, Liangliang and Zhang, Zilun and Zhou, Xinyu and Zhou, Erjin and Liu, Yu},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
	pages        = {13390--13399}
}
@inproceedings{ye2020few,
	title        = {Few-shot learning via embedding adaptation with set-to-set functions},
	author       = {Ye, Han-Jia and Hu, Hexiang and Zhan, De-Chuan and Sha, Fei},
	year         = 2020,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {8808--8817}
}
@article{Ye2020RevisitingTasks,
	title        = {{Revisiting Unsupervised Meta-Learning: Amplifying or Compensating for the Characteristics of Few-Shot Tasks}},
	author       = {Ye, Han-Jia and Han, Lu and Zhan, De-Chuan},
	year         = 2020,
	month        = 11,
	url          = {http://arxiv.org/abs/2011.14663},
	arxivid      = {2011.14663}
}
@inproceedings{ziko2020laplacian,
	title        = {Laplacian regularized few-shot learning},
	author       = {Ziko, Imtiaz and Dolz, Jose and Granger, Eric and Ayed, Ismail Ben},
	year         = 2020,
	booktitle    = {International conference on machine learning},
	pages        = {11660--11670},
	organization = {PMLR}
}
@article{Dosovitskiy2020,
	title        = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	author       = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
	year         = 2020,
	month        = 10,
	doi          = {10.48550/arxiv.2010.11929},
	url          = {https://arxiv.org/abs/2010.11929v2},
	abstract     = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.}
}
@article{Mangla2020,
	title        = {Charting the right manifold: Manifold mixup for few-shot learning},
	author       = {Puneet Mangla and Mayank Singh and Abhishek Sinha and Nupur Kumari and Vineeth N. Balasubramanian and Balaji Krishnamurthy},
	year         = 2020,
	journal      = {Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020},
	pages        = {2207--2216},
	doi          = {10.1109/WACV45572.2020.9093338},
	isbn         = 9781728165530,
	abstract     = {Few-shot learning algorithms aim to learn model parameters capable of adapting to unseen classes with the help of only a few labeled examples. A recent regularization technique - Manifold Mixup focuses on learning a general-purpose representation, robust to small changes in the data distribution. Since the goal of few-shot learning is closely linked to robust representation learning, we study Manifold Mixup in this problem setting. Self-supervised learning is another technique that learns semantically meaningful features, using only the inherent structure of the data. This work investigates the role of learning relevant feature manifold for few-shot tasks using self-supervision and regularization techniques. We observe that regularizing the feature manifold, enriched via self-supervised techniques, with Manifold Mixup significantly improves few-shot learning performance. We show that our proposed method S2M2 beats the current state-of-the-art accuracy on standard few-shot learning datasets like CIFAR-FS, CUB, mini-ImageNet and tiered-ImageNet by 3 - 8\%. Through extensive experimentation, we show that the features learned using our approach generalize to complex few-shot evaluation tasks, cross-domain scenarios and are robust against slight changes to data distribution.}
}
@article{Dwivedi2020,
	title        = {A Generalization of Transformer Networks to Graphs},
	author       = {Vijay Prakash Dwivedi and Xavier Bresson},
	year         = 2020,
	month        = 12,
	doi          = {10.48550/arxiv.2012.09699},
	url          = {https://arxiv.org/abs/2012.09699v2},
	abstract     = {We propose a generalization of transformer neural network architecture for arbitrary graphs. The original transformer was designed for Natural Language Processing (NLP), which operates on fully connected graphs representing all connections between the words in a sequence. Such architecture does not leverage the graph connectivity inductive bias, and can perform poorly when the graph topology is important and has not been encoded into the node features. We introduce a graph transformer with four new properties compared to the standard model. First, the attention mechanism is a function of the neighborhood connectivity for each node in the graph. Second, the positional encoding is represented by the Laplacian eigenvectors, which naturally generalize the sinusoidal positional encodings often used in NLP. Third, the layer normalization is replaced by a batch normalization layer, which provides faster training and better generalization performance. Finally, the architecture is extended to edge feature representation, which can be critical to tasks s.a. chemistry (bond type) or link prediction (entity relationship in knowledge graphs). Numerical experiments on a graph benchmark demonstrate the performance of the proposed graph transformer architecture. This work closes the gap between the original transformer, which was designed for the limited case of line graphs, and graph neural networks, that can work with arbitrary graphs. As our architecture is simple and generic, we believe it can be used as a black box for future applications that wish to consider transformer and graphs.}
}
@article{brown2020language,
	title        = {Language models are few-shot learners},
	author       = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
	year         = 2020,
	journal      = {Advances in neural information processing systems},
	volume       = 33,
	pages        = {1877--1901}
}
@article{gao2020pile,
	title        = {The pile: An 800gb dataset of diverse text for language modeling},
	author       = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and others},
	year         = 2020,
	journal      = {arXiv preprint arXiv:2101.00027}
}
@article{brody2021attentive,
	title        = {How attentive are graph attention networks?},
	author       = {Brody, Shaked and Alon, Uri and Yahav, Eran},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2105.14491}
}
@inproceedings{chen2021exploring,
	title        = {Exploring simple siamese representation learning},
	author       = {Chen, Xinlei and He, Kaiming},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {15750--15758}
}
@inproceedings{chen2021self,
	title        = {Self-supervised learning for few-shot image classification},
	author       = {Chen, Da and Chen, Yuefeng and Li, Yuhong and Mao, Feng and He, Yuan and Xue, Hui},
	year         = 2021,
	booktitle    = {ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
	pages        = {1745--1749},
	organization = {IEEE}
}
@article{Chen2021Self-SupervisedClassification,
	title        = {{Self-Supervised Learning for Few-Shot Image Classification}},
	author       = {Chen, Da and Chen, Yuefeng and Li, Yuhong and Mao, Feng and He, Yuan and Xue, Hui},
	year         = 2021,
	month        = 11,
	publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
	pages        = {1745--1749},
	doi          = {10.1109/icassp39728.2021.9413783},
	url          = {https://arxiv.org/abs/1911.06045v3},
	arxivid      = {1911.06045},
	keywords     = {Cross-domain, Index Terms-Few-shot learning, Metric learning, Self-supervised learning}
}
@article{Cui2021,
	title        = {Parameterless Transductive Feature Re-representation for Few-Shot Learning},
	author       = {Wentao Cui and Yuhong Guo},
	year         = 2021
}
@inproceedings{dwibedi2021little,
	title        = {With a little help from my friends: Nearest-neighbor contrastive learning of visual representations},
	author       = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
	pages        = {9588--9597}
}
@inproceedings{khoi2021sslgan,
	title        = {A Self-supervised GAN for Unsupervised Few-shot Object Recognition},
	author       = {Nguyen, Khoi and Todorovic, Sinisa},
	year         = 2021,
	booktitle    = {2020 25th International Conference on Pattern Recognition (ICPR)},
	volume       = {},
	number       = {},
	pages        = {3225--3231},
	doi          = {10.1109/ICPR48806.2021.9412539}
}
@inproceedings{lee2021meta,
	title        = {Meta-GMVAE: Mixture of Gaussian VAE for Unsupervised Meta-Learning},
	author       = {Lee, Dong Bok and Min, Dongchan and Lee, Seanie and Hwang, Sung Ju},
	year         = 2021,
	booktitle    = {ICLR}
}
@inproceedings{Wang2021,
	title        = {Cross-Domain Few-Shot Classification via Adversarial Task Augmentation},
	author       = {Wang, Haoqing and Deng, Zhi-Hong},
	year         = 2021,
	month        = 8,
	booktitle    = {Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, {IJCAI-21}},
	publisher    = {International Joint Conferences on Artificial Intelligence Organization},
	pages        = {1075--1081},
	doi          = {10.24963/ijcai.2021/149},
	url          = {https://doi.org/10.24963/ijcai.2021/149},
	note         = {Main Track},
	editor       = {Zhi-Hua Zhou}
}
@article{xu2021unsupervised,
	title        = {Unsupervised meta-learning for few-shot learning},
	author       = {Xu, Hui and Wang, Jiaxing and Li, Hao and Ouyang, Deqiang and Shao, Jie},
	year         = 2021,
	journal      = {Pattern Recognition},
	publisher    = {Elsevier},
	volume       = 116,
	pages        = 107951
}
@article{Xu2021UnsupervisedLearning,
	title        = {{Unsupervised meta-learning for few-shot learning}},
	author       = {Xu, Hui and Wang, Jiaxing and Li, Hao and Ouyang, Deqiang and Shao, Jie},
	year         = 2021,
	month        = 8,
	journal      = {Pattern Recognition},
	publisher    = {Elsevier Ltd},
	volume       = 116,
	pages        = 107951,
	doi          = {10.1016/j.patcog.2021.107951},
	issn         = {00313203},
	url          = {https://linkinghub.elsevier.com/retrieve/pii/S0031320321001382},
	keywords     = {Few-shot learning, Meta-learning, Unsupervised learning}
}
@inproceedings{Zhang_2021_ICCV,
	title        = {Shallow Bayesian Meta Learning for Real-World Few-Shot Recognition},
	author       = {Zhang, Xueting and Meng, Debin and Gouk, Henry and Hospedales, Timothy M.},
	year         = 2021,
	month        = {October},
	booktitle    = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
	pages        = {651--660}
}
@inproceedings{Zhang2020RethinkingLearning,
	title        = {Rethinking class relations: Absolute-relative supervised and unsupervised few-shot learning},
	author       = {Zhang, Hongguang and Koniusz, Piotr and Jian, Songlei and Li, Hongdong and Torr, Philip HS},
	year         = 2021,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {9432--9441}
}
@article{zhang2021dive,
	title        = {Dive into Deep Learning},
	author       = {Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J.},
	year         = 2021,
	journal      = {arXiv preprint arXiv:2106.11342}
}
@article{Lindsay2021,
	title        = {Convolutional Neural Networks as a Model of the Visual System: Past, Present, and Future},
	author       = {Grace W. Lindsay},
	year         = 2021,
	month        = 9,
	journal      = {Journal of Cognitive Neuroscience},
	publisher    = {MIT Press},
	volume       = 33,
	pages        = {2017--2031},
	doi          = {10.1162/JOCN_A_01544},
	issn         = {0898-929X},
	url          = {https://direct-mit-edu.tudelft.idm.oclc.org/jocn/article/33/10/2017/97402/Convolutional-Neural-Networks-as-a-Model-of-the},
	abstract     = {Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNs in vision research beyond basic object recognition.},
	issue        = 10,
	pmid         = 32027584
}
@article{Bronstein2021,
	title        = {Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges},
	author       = {Michael M. Bronstein and Joan Bruna and Taco Cohen and Petar Veličković},
	year         = 2021,
	url          = {http://arxiv.org/abs/2104.13478},
	abstract     = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.}
}
@article{Goyal2021,
	title        = {Self-supervised Pretraining of Visual Features in the Wild},
	author       = {Priya Goyal and Mathilde Caron and Benjamin Lefaudeux and Min Xu and Pengchao Wang and Vivek Pai and Mannat Singh and Vitaliy Liptchinsky and Ishan Misra and Armand Joulin and Piotr Bojanowski},
	year         = 2021,
	month        = 3,
	doi          = {10.48550/arxiv.2103.01988},
	url          = {https://arxiv.org/abs/2103.01988v2}
}
@article{GaoContrastiveLearning,
	title        = {Contrastive prototype learning with augmented embeddings for few-shot learning},
	author       = {Gao, Yizhao and Fei, Nanyi and Liu, Guangzhen and Lu, Zhiwu and Xiang, Tao},
	year         = 2021,
	booktitle    = {Uncertainty in Artificial Intelligence},
	pages        = {140--150},
	organization = {PMLR}
}
@inproceedings{hu2021leveraging,
	title        = {Leveraging the feature distribution in transfer-based few-shot learning},
	author       = {Hu, Yuqing and Gripon, Vincent and Pateux, St{\'e}phane},
	year         = 2021,
	booktitle    = {International Conference on Artificial Neural Networks},
	pages        = {487--499},
	organization = {Springer}
}
@misc{chow_2021,
	title        = {Yann Lecun: An early AI prophet},
	author       = {Chow, Rony},
	year         = 2021,
	month        = {Dec},
	journal      = {History of Data Science},
	url          = {https://www.historyofdatascience.com/yann-lecun/}
}
@inproceedings{seidenschwarz2021learning,
	title        = {Learning intra-batch connections for deep metric learning},
	author       = {Seidenschwarz, Jenny Denise and Elezi, Ismail and Leal-Taix{\'e}, Laura},
	year         = 2021,
	booktitle    = {International Conference on Machine Learning},
	pages        = {9410--9421},
	organization = {PMLR}
}
@inproceedings{bardes2022vicreg,
	title        = {{VICR}eg: Variance-Invariance-Covariance Regularization for Self-Supervised Learning},
	author       = {Adrien Bardes and Jean Ponce and Yann LeCun},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=xm6YD62D1Ub}
}
@inproceedings{bateni2022enhancing,
	title        = {Enhancing few-shot image classification with unlabelled examples},
	author       = {Bateni, Peyman and Barber, Jarred and van de Meent, Jan-Willem and Wood, Frank},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
	pages        = {2796--2805}
}
@article{cui2022coarse,
	title        = {Coarse-to-fine pseudo supervision guided meta-task optimization for few-shot object classification},
	author       = {Cui, Yawen and Liao, Qing and Hu, Dewen and An, Wei and Liu, Li},
	year         = 2022,
	journal      = {Pattern Recognition},
	publisher    = {Elsevier},
	volume       = 122,
	pages        = 108296
}
@inproceedings{das2022confess,
	title        = {ConFe{SS}: A Framework for Single Source Cross-Domain Few-Shot Learning},
	author       = {Debasmit Das and Sungrack Yun and Fatih Porikli},
	year         = 2022,
	booktitle    = {International Conference on Learning Representations},
	url          = {https://openreview.net/forum?id=zRJu6mU2BaE}
}
@article{kim2022pure,
	title        = {Pure Transformers are Powerful Graph Learners},
	author       = {Kim, Jinwoo and Nguyen, Tien Dat and Min, Seonwoo and Cho, Sungjun and Lee, Moontae and Lee, Honglak and Hong, Seunghoon},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2207.02505}
}
@inproceedings{li2022ranking,
	title        = {Ranking Distance Calibration for Cross-Domain Few-Shot Learning},
	author       = {Li, Pan and Gong, Shaogang and Wang, Chengjie and Fu, Yanwei},
	year         = 2022,
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages        = {9099--9108}
}
@article{shirekar2022self,
	title        = {Self-Supervised Class-Cognizant Few-Shot Classification},
	author       = {Shirekar, Ojas Kishore and Jamali-Rad, Hadi},
	year         = 2022,
	journal      = {arXiv preprint arXiv:2202.08149}
}
@inproceedings{Yazdanpanah_2022_CVPR,
	title        = {Visual Domain Bridge: A Source-Free Domain Adaptation for Cross-Domain Few-Shot Learning},
	author       = {Yazdanpanah, Moslem and Moradi, Parham},
	year         = 2022,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
	pages        = {2868--2877}
}
@article{Ye2022,
	title        = {{Revisiting Unsupervised Meta-Learning via the Characteristics of Few-Shot Tasks}},
	author       = {Ye, Han-Jia and Han, Lu and Zhan, De-Chuan},
	year         = 2022,
	journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	pages        = {1--1},
	doi          = {10.1109/TPAMI.2022.3179368},
	issn         = {0162-8828},
	url          = {https://ieeexplore.ieee.org/document/9786650/},
	mendeley-groups = {Unsupervised Meta Learning,Unsupervised Meta Learning/FSL-SSL,Unsupervised Meta Learning/Useful-Ideas}
}
@inproceedings{yu2022hybrid,
	title        = {Hybrid Graph Neural Networks for Few-Shot Learning},
	author       = {Yu, Tianyuan and He, Sen and Song, Yi-Zhe and Xiang, Tao},
	year         = 2022,
	booktitle    = {Proceedings of the AAAI Conference on Artificial Intelligence},
	volume       = 36,
	number       = 3,
	pages        = {3179--3187}
}
@article{zhang2022miso,
	title        = {Multi-level Second-order Few-shot Learning},
	author       = {Zhang, Hongguang and Li, Hongdong and Koniusz, Piotr},
	year         = 2022,
	journal      = {IEEE Transactions on Multimedia},
	volume       = {},
	number       = {},
	pages        = {1--1},
	doi          = {10.1109/TMM.2022.3142955}
}
@article{zheng2022unsupervised,
	title        = {Unsupervised few-shot image classification via one-vs-all contrastive learning},
	author       = {Zheng, Zijun and Feng, Xiang and Yu, Huiqun and Li, Xiuquan and Gao, Mengqi},
	year         = 2022,
	journal      = {Applied Intelligence},
	publisher    = {Springer},
	pages        = {1--15}
}
@inproceedings{Zhu_2022_CVPR,
	title        = {EASE: Unsupervised Discriminant Subspace Learning for Transductive Few-Shot Learning},
	author       = {Zhu, Hao and Koniusz, Piotr},
	year         = 2022,
	month        = {June},
	booktitle    = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	pages        = {9078--9088}
}
@article{wagstaff2022universal,
	title        = {Universal approximation of functions on sets},
	author       = {Wagstaff, Edward and Fuchs, Fabian B and Engelcke, Martin and Osborne, Michael A and Posner, Ingmar},
	year         = 2022,
	journal      = {Journal of Machine Learning Research},
	volume       = 23,
	number       = 151,
	pages        = {1--56}
}
@article{marcus2022deep,
	title        = {Deep learning is hitting a wall},
	author       = {Marcus, Gary},
	year         = 2022,
	journal      = {Nautilus, Accessed},
	pages        = {03--11}
}
@article{cohen2022lamda,
	title        = {LaMDA: Language Models for Dialog Applications},
	author       = {Cohen, Aaron Daniel and Roberts, Adam and Molina, Alejandra and Butryna, Alena and Jin, Alicia and Kulshreshtha, Apoorv and Hutchinson, Ben and Zevenbergen, Ben and Aguera-Arcas, Blaise Hilary and Chang, Chung-ching and others},
	year         = 2022
}